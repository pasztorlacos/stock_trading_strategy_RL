{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_stock=quandl.get('WIKI/AAPL', start_date=\"2014-01-01\", end_date=\"2019-06-20\", )\n",
    "msf_stock=quandl.get('WIKI/MSFT', start_date=\"2014-01-01\", end_date=\"2019-06-20\")\n",
    "apl_open = apl_stock[\"Open\"].values\n",
    "apl_close = apl_stock[\"Close\"].values\n",
    "msf_open = msf_stock[\"Open\"].values\n",
    "msf_close = msf_stock[\"Close\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msf_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(values, title):\n",
    "    plt.plot(range(0, len(values)), values)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(msf_open, 'msf_open original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened - stock split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:  9 June 2014: AAPL stock was split in the ratio of 1:7.\n",
    "\n",
    "Solution: divide everything before element 108 in the stock array by 7 to correct for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_close[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open adjusted for stock split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove general upwards trend.\n",
    "\n",
    "• De-trend the data.\n",
    "\n",
    "• Otherwise can't expect model to learn to trade MSFT and AAPL when they are on different scales.\n",
    "\n",
    "• Also want the model to learn the fundamentals of the stock signal - buy if it's going to rise.\n",
    "\n",
    "• Without removing trend, the model might simply learn to hold - i.e. buy at the start and hold until the end in upward trends. WHile beneficial, we want more from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal processing module - fit linear LSQ model to data and subtract it.\n",
    "msf_open = signal.detrend(msf_open)\n",
    "msf_close = signal.detrend(msf_close)\n",
    "\n",
    "plot_data(msf_open, 'msf_open detrended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open = signal.detrend(apl_open)\n",
    "apl_close = signal.detrend(apl_close)\n",
    "\n",
    "plot_data(apl_open, 'apl_open detrended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove negative values\n",
    "\n",
    "• Doesn't make sense to buy a share at a negative value.\n",
    "\n",
    "• The model will learn to maximize reward anyway --> can simply shift values up by a constant number to ensure strictly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apl_open.min())\n",
    "print(apl_close.min())\n",
    "print(msf_open.min())\n",
    "print(msf_close.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose +35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open += 35.\n",
    "apl_close += 35.\n",
    "msf_open += 35.\n",
    "msf_close += 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"apl_msf_open_close.pkl\", \"wb+\") as f:\n",
    "    pickle.dump({\"ao\":apl_open, \"ac\": apl_close, \"mo\": msf_open, \"mc\": msf_close}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the data is already saved, skip the above and load it here instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"apl_msf_open_close.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    \n",
    "apl_open = d[\"ao\"]\n",
    "apl_close = d[\"ac\"]\n",
    "msf_open = d[\"mo\"]\n",
    "msf_close = d[\"mc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open reloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the environment\n",
    "\n",
    "* For starting cash, we can't use a dollar value because of the transformed data. After shifting earlier, we know the mean of each opening price should be 35, so I'm starting the agent off with enough cash to buy ~2.5 shares.\n",
    "* This starting cash is the mean starting cash; it can be randomized by altering the std parameter\n",
    "* action space = 0 buy apple, 1 sell apple, 2 do nothing, 3 buy msft, 4 sell msft, quantity. eg. [0, 100]\n",
    "* obs space: apl shares, msft shares, cash in bank [2], today apl open [3], today msf open [4], portfolio value [5], 5 day window [6apl, 7msf] = 8\n",
    "* If bot gets to the end with more than one of each share, we give it a bonus for having a diversified portfolio!\n",
    "* Buys and sells attract a 10% brokerage fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment():\n",
    "\n",
    "    def __init__(self, starting_cash_mean=200., max_stride=5, series_length=208, starting_point=1, randomize_cash_std=0, \\\n",
    "                 starting_shares_mean=0., randomize_shares_std=0., inaction_penalty=0.):\n",
    "        self.starting_shares_mean = starting_shares_mean\n",
    "        self.randomize_shares_std = randomize_shares_std\n",
    "        self.starting_cash_mean = starting_cash_mean\n",
    "        self.randomize_cash_std = randomize_cash_std\n",
    "        \n",
    "        # self.state = torch.FloatTensor(torch.zeros(8)).cuda()\n",
    "        self.state = torch.FloatTensor(torch.zeros(8))\n",
    "        \n",
    "        self.starting_cash = max(int(np.random.normal(self.starting_cash_mean, self.randomize_cash_std)), 0.)\n",
    "        \n",
    "        self.series_length = series_length\n",
    "        self.starting_point = starting_point\n",
    "        self.cur_timestep = self.starting_point\n",
    "        \n",
    "        self.state[0] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n",
    "        self.state[1] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n",
    "        self.starting_portfolio_value = self.portfolio_value()\n",
    "        self.state[2] = self.starting_cash\n",
    "        self.state[3] = apl_open[self.cur_timestep]\n",
    "        self.state[4] = msf_open[self.cur_timestep]\n",
    "        self.state[5] = self.starting_portfolio_value\n",
    "        self.state[6] = self.five_day_window()[0]\n",
    "        self.state[7] = self.five_day_window()[1]\n",
    "        \n",
    "        self.max_stride = max_stride\n",
    "        self.stride = self.max_stride # no longer varying it\n",
    "        \n",
    "        self.done = False\n",
    "        self.diversification_bonus = 1.\n",
    "        self.inaction_penalty = inaction_penalty\n",
    "    \n",
    "    def portfolio_value(self):\n",
    "        return (self.state[0] * apl_close[self.cur_timestep]) + (self.state[1] * msf_close[self.cur_timestep]) + self.state[2]\n",
    "    \n",
    "    def next_opening_price(self):\n",
    "        step = self.cur_timestep + self.stride\n",
    "        return [apl_open[step], msf_open[step]]\n",
    "    \n",
    "    def five_day_window(self):\n",
    "        step = self.cur_timestep\n",
    "        if step < 5:\n",
    "            return [apl_open[0], msf_open[0]]\n",
    "        apl5 = apl_open[step-5:step].mean()\n",
    "        msf5 = msf_open[step-5:step].mean()\n",
    "        return [apl5, msf5]\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = [action, 1.]\n",
    "        cur_timestep = self.cur_timestep\n",
    "        ts_left = self.series_length - (cur_timestep - self.starting_point)\n",
    "        retval = None\n",
    "        cur_value = self.portfolio_value()\n",
    "        gain = cur_value - self.starting_portfolio_value\n",
    "        \n",
    "        if cur_timestep >= self.starting_point + (self.series_length * self.stride):\n",
    "            new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                        cur_value, *self.five_day_window()]\n",
    "            self.state = new_state\n",
    "            bonus = 0.\n",
    "            if self.state[0] > 0 and self.state[1] > 0:\n",
    "                bonus = self.diversification_bonus\n",
    "            return new_state, cur_value + bonus + gain, True, { \"msg\": \"done\"}\n",
    "        \n",
    "        if action[0] == 2:\n",
    "            new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                    cur_value, *self.five_day_window()]\n",
    "            self.state = new_state\n",
    "            retval = new_state, -self.inaction_penalty-ts_left +gain, False, { \"msg\": \"nothing\" }\n",
    "            \n",
    "        if action[0] == 0:\n",
    "            if action[1] * apl_open[cur_timestep] > self.state[2]:\n",
    "                new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                        cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, -ts_left+gain/2, True, { \"msg\": \"bankrupted self\"}\n",
    "            else:\n",
    "                apl_shares = self.state[0] + action[1]\n",
    "                cash_spent = action[1] * apl_open[cur_timestep] * 1.1\n",
    "                new_state = [apl_shares, self.state[1], self.state[2] - cash_spent, *self.next_opening_price(), \\\n",
    "                       cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"bought AAPL\"}\n",
    "                \n",
    "        if action[0] == 3:\n",
    "            if action[1] * msf_open[cur_timestep] > self.state[2]:\n",
    "                new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                        cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval =  new_state, -ts_left+gain/2, True, { \"msg\": \"bankrupted self\"}\n",
    "            else:\n",
    "                msf_shares = self.state[1] + action[1]\n",
    "                cash_spent = action[1] * msf_open[cur_timestep] * 1.1\n",
    "                new_state = [self.state[0], msf_shares, self.state[2] - cash_spent, *self.next_opening_price(), \\\n",
    "                       cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"bought MSFT\"}\n",
    "        \n",
    "\n",
    "        if action[0] == 1:\n",
    "            if action[1] > self.state[0]:\n",
    "                new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                        cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, -ts_left+gain/2, True, { \"msg\": \"sold more than have\"}\n",
    "            else:\n",
    "                apl_shares = self.state[0] - action[1]\n",
    "                cash_gained = action[1] * apl_open[cur_timestep] * 0.9\n",
    "                new_state = [apl_shares, self.state[1], self.state[2] + cash_gained, *self.next_opening_price(), \\\n",
    "                       cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"sold AAPL\"}\n",
    "                \n",
    "        if action[0] == 4:\n",
    "            if action[1] > self.state[1]:\n",
    "                new_state = [self.state[0], self.state[1], self.state[2], *self.next_opening_price(), \\\n",
    "                        cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, -ts_left+gain/2, True, { \"msg\": \"sold more than have\"}\n",
    "            else:\n",
    "                msf_shares = self.state[1] - action[1]\n",
    "                cash_gained = action[1] * msf_open[cur_timestep] * 0.9\n",
    "                new_state = [self.state[0], msf_shares, self.state[2] + cash_gained, *self.next_opening_price(), \\\n",
    "                       cur_value, *self.five_day_window()]\n",
    "                self.state = new_state\n",
    "                retval = new_state, self.inaction_penalty-ts_left+gain, False, { \"msg\": \"sold MSFT\"}\n",
    "                \n",
    "        self.cur_timestep += self.stride\n",
    "        return retval\n",
    "    \n",
    "    def reset(self):\n",
    "        # self.state = torch.FloatTensor(torch.zeros(8)).cuda()\n",
    "        self.state = torch.FloatTensor(torch.zeros(8))\n",
    "        self.starting_cash = max(int(np.random.normal(self.starting_cash_mean, self.randomize_cash_std)), 0.)\n",
    "        self.cur_timestep = self.starting_point\n",
    "        self.state[0] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n",
    "        self.state[1] = max(int(np.random.normal(self.starting_shares_mean, self.randomize_shares_std)), 0.)\n",
    "        self.state[2] = self.starting_cash\n",
    "        self.state[3] = apl_open[self.cur_timestep]\n",
    "        self.state[4] = msf_open[self.cur_timestep]\n",
    "        self.starting_portfolio_value = self.portfolio_value()\n",
    "        self.state[5] = self.starting_portfolio_value\n",
    "        self.state[6] = self.five_day_window()[0]\n",
    "        self.state[7] = self.five_day_window()[1]       \n",
    "        self.done = False\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.input_layer = nn.Linear(8, 128)\n",
    "        self.hidden_1 = nn.Linear(128, 128)\n",
    "        self.hidden_2 = nn.Linear(32,31)\n",
    "        # self.hidden_state = torch.tensor(torch.zeros(2,1,32)).cuda()\n",
    "        self.hidden_state = torch.tensor(torch.zeros(2,1,32))\n",
    "        self.rnn = nn.GRU(128, 32, 2)\n",
    "        self.action_head = nn.Linear(31, 5)\n",
    "        self.value_head = nn.Linear(31, 1)\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        # self.hidden_state = torch.tensor(torch.zeros(2,1,32)).cuda()\n",
    "        self.hidden_state = torch.tensor(torch.zeros(2,1,32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = torch.tensor(x).cuda()\n",
    "        x = torch.tensor(x)\n",
    "        x = torch.sigmoid(self.input_layer(x))\n",
    "        x = torch.tanh(self.hidden_1(x))\n",
    "        x, self.hidden_state = self.rnn(x.view(1,-1,128), self.hidden_state.data)\n",
    "        x = F.relu(self.hidden_2(x.squeeze()))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "    \n",
    "    def act(self, state):\n",
    "        probs, state_value = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        # if action == 1 and env.state[0] < 1: action = torch.LongTensor([2]).squeeze().cuda()\n",
    "        # if action == 4 and env.state[1] < 1: action = torch.LongTensor([2]).squeeze().cuda()\n",
    "        if action == 1 and env.state[0] < 1: action = torch.LongTensor([2]).squeeze()\n",
    "        if action == 4 and env.state[1] < 1: action = torch.LongTensor([2]).squeeze()\n",
    "        self.saved_actions.append((m.log_prob(action), state_value))\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "# model = Policy().cuda()\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# In case you're running this a second time with the same model, delete the gradients\n",
    "del model.rewards[:]\n",
    "del model.saved_actions[:]\n",
    "\n",
    "gamma = 0.9\n",
    "log_interval = 60\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + (gamma * R)\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    epsilon = (torch.rand(1) / 1e4) - 5e-5\n",
    "    # With different architectures, I found the following standardization step sometimes\n",
    "    # helpful, sometimes unhelpful.\n",
    "    # rewards = (rewards - rewards.mean()) / (rewards.std(unbiased=False) + epsilon)\n",
    "    # Alternatively, comment it out and use the following line instead:\n",
    "    rewards += epsilon\n",
    "    \n",
    "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
    "        # reward = torch.tensor(r - value.item()).cuda()\n",
    "        reward = torch.tensor(r - value.item())\n",
    "        policy_losses.append(-log_prob * reward)\n",
    "        # value_losses.append(F.smooth_l1_loss(value, torch.tensor([r]).cuda()))\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss = torch.clamp(loss, -1e-5, 1e5)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "running_reward = 0\n",
    "for episode in range(0, 4000):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    msg = None\n",
    "    while not done:\n",
    "        action = model.act(state)\n",
    "        state, reward, done, msg = env.step(action)\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    running_reward = running_reward * (1 - 1/log_interval) + reward * (1/log_interval)\n",
    "    finish_episode()\n",
    "    # Resetting the hidden state seems unnecessary - it's effectively random from the previous\n",
    "    # episode anyway, more random than a bunch of zeros.\n",
    "    # model.reset_hidden()\n",
    "    if msg[\"msg\"] == \"done\" and env.portfolio_value() > env.starting_portfolio_value * 1.1 and running_reward > 500:\n",
    "        print(\"Early Stopping: \" + str(int(reward)))\n",
    "        break\n",
    "    if episode % log_interval == 0:\n",
    "        print(\"\"\"Episode {}: started at {:.1f}, finished at {:.1f} because {} @ t={}, \\\n",
    "last reward {:.1f}, running reward {:.1f}\"\"\".format(episode, env.starting_portfolio_value, \\\n",
    "              env.portfolio_value(), msg[\"msg\"], env.cur_timestep, reward, running_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how it does in practice, on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "\n",
    "total_rewards = 0\n",
    "total_profits = 0\n",
    "failed_goes = 0\n",
    "num_goes = 50\n",
    "\n",
    "for j in range(num_goes):\n",
    "    env.reset()\n",
    "    reward_this_go = -1e8\n",
    "    for i in range(0,env.series_length + 1):\n",
    "        action = model.act(env.state)\n",
    "        next_state, reward, done, msg = env.step(action)\n",
    "        if msg[\"msg\"] == \"done\":\n",
    "            reward_this_go = env.portfolio_value()\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "    total_profits += (env.portfolio_value() - env.starting_portfolio_value) / env.starting_portfolio_value\n",
    "    if reward_this_go == -1e8:\n",
    "        failed_goes += 1\n",
    "    else:\n",
    "        total_rewards += reward_this_go\n",
    "\n",
    "if failed_goes == num_goes:\n",
    "    print(\"Failed all\")\n",
    "else:\n",
    "    print(\"Failed goes: {} / {}, Avg Rewards per successful game: {}\".format(failed_goes, num_goes, total_rewards / (num_goes - failed_goes)))\n",
    "    print(\"Avg % profit per game: {}\".format(total_profits / num_goes))\n",
    "    print(\"Avg % profit per finished game: {}\".format(total_profits / (num_goes - failed_goes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And here's how a sample trading run might look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "env.reset()\n",
    "print(\"starting portfolio value {}\".format(env.portfolio_value()))\n",
    "for i in range(0,env.series_length + 1):\n",
    "    action = model.act(env.state)\n",
    "    next_state, reward, done, msg = env.step(action)\n",
    "    if msg[\"msg\"] == 'bankrupted self':\n",
    "        print('bankrupted self by 1')\n",
    "        break\n",
    "    if msg[\"msg\"] == 'sold more than have':\n",
    "        print('sold more than have by 1')\n",
    "        break\n",
    "    print(\"{}, have {} aapl and {} msft and {} cash\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n",
    "    if msg[\"msg\"] == \"done\":\n",
    "        print(next_state, reward)\n",
    "        print(\"total portfolio value {}\".format(env.portfolio_value()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open_orig = apl_stock[\"Open\"].values\n",
    "apl_close_orig = apl_stock[\"Close\"].values\n",
    "msf_open_orig = msf_stock[\"Open\"].values\n",
    "msf_close_orig = msf_stock[\"Close\"].values\n",
    "apl_open_orig[:108] /= 7\n",
    "apl_close_orig[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "env.reset()\n",
    "complete_game = False\n",
    "while not complete_game:\n",
    "    bought_apl_at = []\n",
    "    bought_msf_at = []\n",
    "    sold_apl_at = []\n",
    "    sold_msf_at = []\n",
    "    bought_apl_at_orig = []\n",
    "    bought_msf_at_orig = []\n",
    "    sold_apl_at_orig = []\n",
    "    sold_msf_at_orig = []\n",
    "    nothing_at = []\n",
    "    ba_action_times = []\n",
    "    bm_action_times = []\n",
    "    sa_action_times = []\n",
    "    sm_action_times = []\n",
    "    n_action_times = []\n",
    "    starting_val = env.starting_portfolio_value\n",
    "    print(\"Starting portfolio value: {}\".format(starting_val))\n",
    "    for i in range(0,env.series_length + 1):\n",
    "        action = model.act(env.state)\n",
    "        if action == 0:\n",
    "            bought_apl_at.append(apl_open[env.cur_timestep])\n",
    "            bought_apl_at_orig.append(apl_open_orig[env.cur_timestep])\n",
    "            ba_action_times.append(env.cur_timestep)\n",
    "        if action == 1:\n",
    "            sold_apl_at.append(apl_close[env.cur_timestep])\n",
    "            sold_apl_at_orig.append(apl_close_orig[env.cur_timestep])\n",
    "            sa_action_times.append(env.cur_timestep)\n",
    "        if action == 2:\n",
    "            nothing_at.append(35)\n",
    "            n_action_times.append(env.cur_timestep)\n",
    "        if action == 3:\n",
    "            bought_msf_at.append(msf_open[env.cur_timestep])\n",
    "            bought_msf_at_orig.append(msf_open_orig[env.cur_timestep])\n",
    "            bm_action_times.append(env.cur_timestep)\n",
    "        if action == 4:\n",
    "            sold_msf_at.append(msf_close[env.cur_timestep])\n",
    "            sold_msf_at_orig.append(msf_close_orig[env.cur_timestep])\n",
    "            sm_action_times.append(env.cur_timestep)\n",
    "        next_state, reward, done, msg = env.step(action)\n",
    "        if msg[\"msg\"] == 'bankrupted self':\n",
    "            env.reset()\n",
    "            break\n",
    "        if msg[\"msg\"] == 'sold more than have':\n",
    "            env.reset()\n",
    "            break\n",
    "        if msg[\"msg\"] == \"done\":\n",
    "            print(\"{}, have {} aapl and {} msft and {} cash\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n",
    "            val = env.portfolio_value()\n",
    "            print(\"Finished portfolio value {}\".format(val))\n",
    "            if val > starting_val * 1.1: complete_game = True\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,5))\n",
    "apl = plt.subplot(121)\n",
    "msf = plt.subplot(122)\n",
    "apl.plot(range(0, len(apl_open)), apl_open)\n",
    "msf.plot(range(0, len(msf_open)), msf_open)\n",
    "apl.plot(ba_action_times, bought_apl_at, \"ro\")\n",
    "apl.plot(sa_action_times, sold_apl_at, \"go\")\n",
    "apl.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(bm_action_times, bought_msf_at, \"ro\")\n",
    "msf.plot(sm_action_times, sold_msf_at, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,5))\n",
    "apl = plt.subplot(121)\n",
    "msf = plt.subplot(122)\n",
    "apl.plot(range(0, len(apl_open_orig)), apl_open_orig)\n",
    "msf.plot(range(0, len(msf_open_orig)), msf_open_orig)\n",
    "apl.plot(ba_action_times, bought_apl_at_orig, \"ro\")\n",
    "apl.plot(sa_action_times, sold_apl_at_orig, \"go\")\n",
    "apl.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(bm_action_times, bought_msf_at_orig, \"ro\")\n",
    "msf.plot(sm_action_times, sold_msf_at_orig, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-conda",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
