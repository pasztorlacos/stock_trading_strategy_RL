{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_stock=quandl.get('WIKI/AAPL', start_date=\"2014-01-01\", end_date=\"2019-06-20\", )\n",
    "msf_stock=quandl.get('WIKI/MSFT', start_date=\"2014-01-01\", end_date=\"2019-06-20\")\n",
    "apl_open = apl_stock[\"Open\"].values\n",
    "apl_close = apl_stock[\"Close\"].values\n",
    "msf_open = msf_stock[\"Open\"].values\n",
    "msf_close = msf_stock[\"Close\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msf_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(values, title):\n",
    "    plt.plot(range(0, len(values)), values)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(msf_open, 'msf_open original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened - stock split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:  9 June 2014: AAPL stock was split in the ratio of 1:7.\n",
    "\n",
    "Solution: divide everything before element 108 in the stock array by 7 to correct for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_close[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open adjusted for stock split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove general upwards trend.\n",
    "\n",
    "• De-trend the data.\n",
    "\n",
    "• Otherwise can't expect model to learn to trade MSFT and AAPL when they are on different scales.\n",
    "\n",
    "• Also want the model to learn the fundamentals of the stock signal - buy if it's going to rise.\n",
    "\n",
    "• Without removing trend, the model might simply learn to hold - i.e. buy at the start and hold until the end in upward trends. WHile beneficial, we want more from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal processing module - fit linear LSQ model to data and subtract it.\n",
    "msf_open = signal.detrend(msf_open)\n",
    "msf_close = signal.detrend(msf_close)\n",
    "\n",
    "plot_data(msf_open, 'msf_open detrended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open = signal.detrend(apl_open)\n",
    "apl_close = signal.detrend(apl_close)\n",
    "\n",
    "plot_data(apl_open, 'apl_open detrended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove negative values\n",
    "\n",
    "• Doesn't make sense to buy a share at a negative value.\n",
    "\n",
    "• The model will learn to maximize reward anyway --> can simply shift values up by a constant number to ensure strictly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apl_open.min())\n",
    "print(apl_close.min())\n",
    "print(msf_open.min())\n",
    "print(msf_close.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose +35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open += 35.\n",
    "apl_close += 35.\n",
    "msf_open += 35.\n",
    "msf_close += 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"apl_msf_open_close.pkl\", \"wb+\") as f:\n",
    "    pickle.dump({\"ao\":apl_open, \"ac\": apl_close, \"mo\": msf_open, \"mc\": msf_close}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the data is already saved, skip the above and load it here instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"apl_msf_open_close.pkl\", \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "    \n",
    "apl_open = d[\"ao\"]\n",
    "apl_close = d[\"ac\"]\n",
    "msf_open = d[\"mo\"]\n",
    "msf_close = d[\"mc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(apl_open, 'apl_open reloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "# model = Policy().cuda()\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# In case you're running this a second time with the same model, delete the gradients\n",
    "del model.rewards[:]\n",
    "del model.saved_actions[:]\n",
    "\n",
    "gamma = 0.9\n",
    "log_interval = 60\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + (gamma * R)\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    epsilon = (torch.rand(1) / 1e4) - 5e-5\n",
    "    # With different architectures, I found the following standardization step sometimes\n",
    "    # helpful, sometimes unhelpful.\n",
    "    # rewards = (rewards - rewards.mean()) / (rewards.std(unbiased=False) + epsilon)\n",
    "    # Alternatively, comment it out and use the following line instead:\n",
    "    rewards += epsilon\n",
    "    \n",
    "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
    "        # reward = torch.tensor(r - value.item()).cuda()\n",
    "        reward = torch.tensor(r - value.item())\n",
    "        policy_losses.append(-log_prob * reward)\n",
    "        # value_losses.append(F.smooth_l1_loss(value, torch.tensor([r]).cuda()))\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss = torch.clamp(loss, -1e-5, 1e5)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "running_reward = 0\n",
    "for episode in range(0, 4000):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    msg = None\n",
    "    while not done:\n",
    "        action = model.act(state)\n",
    "        state, reward, done, msg = env.step(action)\n",
    "        model.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    running_reward = running_reward * (1 - 1/log_interval) + reward * (1/log_interval)\n",
    "    finish_episode()\n",
    "    # Resetting the hidden state seems unnecessary - it's effectively random from the previous\n",
    "    # episode anyway, more random than a bunch of zeros.\n",
    "    # model.reset_hidden()\n",
    "    if msg[\"msg\"] == \"done\" and env.portfolio_value() > env.starting_portfolio_value * 1.1 and running_reward > 500:\n",
    "        print(\"Early Stopping: \" + str(int(reward)))\n",
    "        break\n",
    "    if episode % log_interval == 0:\n",
    "        print(\"\"\"Episode {}: started at {:.1f}, finished at {:.1f} because {} @ t={}, \\\n",
    "last reward {:.1f}, running reward {:.1f}\"\"\".format(episode, env.starting_portfolio_value, \\\n",
    "              env.portfolio_value(), msg[\"msg\"], env.cur_timestep, reward, running_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how it does in practice, on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "\n",
    "total_rewards = 0\n",
    "total_profits = 0\n",
    "failed_goes = 0\n",
    "num_goes = 50\n",
    "\n",
    "for j in range(num_goes):\n",
    "    env.reset()\n",
    "    reward_this_go = -1e8\n",
    "    for i in range(0,env.series_length + 1):\n",
    "        action = model.act(env.state)\n",
    "        next_state, reward, done, msg = env.step(action)\n",
    "        if msg[\"msg\"] == \"done\":\n",
    "            reward_this_go = env.portfolio_value()\n",
    "            break\n",
    "        if done:\n",
    "            break\n",
    "    total_profits += (env.portfolio_value() - env.starting_portfolio_value) / env.starting_portfolio_value\n",
    "    if reward_this_go == -1e8:\n",
    "        failed_goes += 1\n",
    "    else:\n",
    "        total_rewards += reward_this_go\n",
    "\n",
    "if failed_goes == num_goes:\n",
    "    print(\"Failed all\")\n",
    "else:\n",
    "    print(\"Failed goes: {} / {}, Avg Rewards per successful game: {}\".format(failed_goes, num_goes, total_rewards / (num_goes - failed_goes)))\n",
    "    print(\"Avg % profit per game: {}\".format(total_profits / num_goes))\n",
    "    print(\"Avg % profit per finished game: {}\".format(total_profits / (num_goes - failed_goes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And here's how a sample trading run might look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "env.reset()\n",
    "print(\"starting portfolio value {}\".format(env.portfolio_value()))\n",
    "for i in range(0,env.series_length + 1):\n",
    "    action = model.act(env.state)\n",
    "    next_state, reward, done, msg = env.step(action)\n",
    "    if msg[\"msg\"] == 'bankrupted self':\n",
    "        print('bankrupted self by 1')\n",
    "        break\n",
    "    if msg[\"msg\"] == 'sold more than have':\n",
    "        print('sold more than have by 1')\n",
    "        break\n",
    "    print(\"{}, have {} aapl and {} msft and {} cash\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n",
    "    if msg[\"msg\"] == \"done\":\n",
    "        print(next_state, reward)\n",
    "        print(\"total portfolio value {}\".format(env.portfolio_value()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_open_orig = apl_stock[\"Open\"].values\n",
    "apl_close_orig = apl_stock[\"Close\"].values\n",
    "msf_open_orig = msf_stock[\"Open\"].values\n",
    "msf_close_orig = msf_stock[\"Close\"].values\n",
    "apl_open_orig[:108] /= 7\n",
    "apl_close_orig[:108] /= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironment(max_stride=4, series_length=250, starting_cash_mean=1000, randomize_cash_std=100, starting_shares_mean=100, randomize_shares_std=10)\n",
    "env.reset()\n",
    "complete_game = False\n",
    "while not complete_game:\n",
    "    bought_apl_at = []\n",
    "    bought_msf_at = []\n",
    "    sold_apl_at = []\n",
    "    sold_msf_at = []\n",
    "    bought_apl_at_orig = []\n",
    "    bought_msf_at_orig = []\n",
    "    sold_apl_at_orig = []\n",
    "    sold_msf_at_orig = []\n",
    "    nothing_at = []\n",
    "    ba_action_times = []\n",
    "    bm_action_times = []\n",
    "    sa_action_times = []\n",
    "    sm_action_times = []\n",
    "    n_action_times = []\n",
    "    starting_val = env.starting_portfolio_value\n",
    "    print(\"Starting portfolio value: {}\".format(starting_val))\n",
    "    for i in range(0,env.series_length + 1):\n",
    "        action = model.act(env.state)\n",
    "        if action == 0:\n",
    "            bought_apl_at.append(apl_open[env.cur_timestep])\n",
    "            bought_apl_at_orig.append(apl_open_orig[env.cur_timestep])\n",
    "            ba_action_times.append(env.cur_timestep)\n",
    "        if action == 1:\n",
    "            sold_apl_at.append(apl_close[env.cur_timestep])\n",
    "            sold_apl_at_orig.append(apl_close_orig[env.cur_timestep])\n",
    "            sa_action_times.append(env.cur_timestep)\n",
    "        if action == 2:\n",
    "            nothing_at.append(35)\n",
    "            n_action_times.append(env.cur_timestep)\n",
    "        if action == 3:\n",
    "            bought_msf_at.append(msf_open[env.cur_timestep])\n",
    "            bought_msf_at_orig.append(msf_open_orig[env.cur_timestep])\n",
    "            bm_action_times.append(env.cur_timestep)\n",
    "        if action == 4:\n",
    "            sold_msf_at.append(msf_close[env.cur_timestep])\n",
    "            sold_msf_at_orig.append(msf_close_orig[env.cur_timestep])\n",
    "            sm_action_times.append(env.cur_timestep)\n",
    "        next_state, reward, done, msg = env.step(action)\n",
    "        if msg[\"msg\"] == 'bankrupted self':\n",
    "            env.reset()\n",
    "            break\n",
    "        if msg[\"msg\"] == 'sold more than have':\n",
    "            env.reset()\n",
    "            break\n",
    "        if msg[\"msg\"] == \"done\":\n",
    "            print(\"{}, have {} aapl and {} msft and {} cash\".format(msg[\"msg\"], next_state[0], next_state[1], next_state[2]))\n",
    "            val = env.portfolio_value()\n",
    "            print(\"Finished portfolio value {}\".format(val))\n",
    "            if val > starting_val * 1.1: complete_game = True\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,5))\n",
    "apl = plt.subplot(121)\n",
    "msf = plt.subplot(122)\n",
    "apl.plot(range(0, len(apl_open)), apl_open)\n",
    "msf.plot(range(0, len(msf_open)), msf_open)\n",
    "apl.plot(ba_action_times, bought_apl_at, \"ro\")\n",
    "apl.plot(sa_action_times, sold_apl_at, \"go\")\n",
    "apl.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(bm_action_times, bought_msf_at, \"ro\")\n",
    "msf.plot(sm_action_times, sold_msf_at, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,5))\n",
    "apl = plt.subplot(121)\n",
    "msf = plt.subplot(122)\n",
    "apl.plot(range(0, len(apl_open_orig)), apl_open_orig)\n",
    "msf.plot(range(0, len(msf_open_orig)), msf_open_orig)\n",
    "apl.plot(ba_action_times, bought_apl_at_orig, \"ro\")\n",
    "apl.plot(sa_action_times, sold_apl_at_orig, \"go\")\n",
    "apl.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(n_action_times, nothing_at, \"yx\")\n",
    "msf.plot(bm_action_times, bought_msf_at_orig, \"ro\")\n",
    "msf.plot(sm_action_times, sold_msf_at_orig, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-conda",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
